# Course 2: Improving Deep Neural Networks  
Part of the [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew Ng.

---

## Course Overview
This course focuses on practical techniques to improve deep neural networks:

1. **Practical Aspects of Deep Learning**
   - Train/dev/test sets
   - Bias-variance analysis
   - Regularization (L2, dropout)
   - Gradient checking

2. **Optimization Algorithms**
   - Mini-batch gradient descent
   - Momentum
   - RMSProp
   - Adam optimizer
   - Learning rate decay

3. **Hyperparameter Tuning, Batch Normalization, and Programming Frameworks**
   - Random vs. grid search
   - Normalizing activations with Batch Norm
   - Softmax and cross-entropy
   - TensorFlow basics (v1.x style in the course)

---

## Assignments in This Repository
- **Initialization**: Implement different weight initialization techniques and observe their effects.  
- **Regularization**: Apply L2 and dropout to reduce overfitting.  
- **Gradient Checking**: Verify correctness of backpropagation.  
- **Optimization**: Implement and compare Momentum, RMSProp, and Adam.  
- **TensorFlow Basics**: Build and train a simple neural network using TensorFlow.
